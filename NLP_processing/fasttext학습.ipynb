{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 본 페이지의 목표\n",
    "\n",
    "-gensim에서 제공하는 fasttext 모델에 없는 단어를 학습시킨다.\n",
    "\n",
    "-없는 단어를 학습하기 위해 크롤링을 하여 context를 모은다.\n",
    "\n",
    "-크롤링한 context를 fasttext에 학습시킨다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import codecs\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    STABILISATION\n",
      "1           MOOHAN\n",
      "2            INLAY\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "list_items = pd.read_csv('C:/Users/admin/Desktop/NLP/description/skipgram/오타확인/total_typo_check_list.txt',header = None, delimiter=\"\\t\", quoting=csv.QUOTE_NONE, encoding='utf-8')\n",
    "list_items = list_items[0]\n",
    "print(list_items[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 기존에 존재하지 않은 단어 크롤링! \n",
    "\n",
    "크롤링을 통해 source,target description에 미포함된 단어의 context를 만든다.\n",
    "\n",
    "만드는 규칙 따위 없다. 그냥 단어에 상관없이 문장을 다 더할것이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'INFOMED'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_items[212]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re # 한글 분리\n",
    "hangul = re.compile('[^가-힣]+')\n",
    "english = re.compile('[^a-zA-Z0-9.,:-]+')\n",
    "not_english = re.compile('[a-zA-Z0-9.,]+')\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "options.add_argument('window-size=1920x1080')\n",
    "options.add_argument(\"disable-gpu\")\n",
    "options.add_argument('--dns-prefetch-disable')\n",
    "# options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML like Gecko) Chrome/51.0.2704.79 Safari/537.36 Edge/14.14931\")\n",
    "#크롬 드라이버 경로를 위치시킨다\n",
    "driver = webdriver.Chrome('C:/Users/admin/Desktop/NLP/chromedriver_v71/chromedriver', options=options)\n",
    "# crawl_txt = []\n",
    "driver.implicitly_wait(3)\n",
    "# ex =['COFLEXINTERSPINOUSIMPLANTS']#list_items\n",
    "for idx,search_item in enumerate(list_items[527:]):\n",
    "    driver = webdriver.Chrome('C:/Users/admin/Desktop/NLP/chromedriver_v71/chromedriver', options=options)\n",
    "    driver.implicitly_wait(3)\n",
    "    url = \"https://www.google.com\"\n",
    "    driver.get(url)\n",
    "    elem = driver.find_element_by_xpath('//*[@id=\"tsf\"]/div[2]/div/div[1]/div/div[1]/input')\n",
    "    elem.send_keys(str(search_item))\n",
    "    elem.submit()\n",
    "    driver.implicitly_wait(5) # 브라우저 검색 최소 지연시간 \n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    notices = soup.select('#fprsl > em')\n",
    "    no_result = soup.select('#topstuff > div > div > p') \n",
    "    div_group=[]\n",
    "\n",
    "    \n",
    "    if(len(no_result)>0):\n",
    "        print('검색결과 없음')\n",
    "        continue\n",
    "                             \n",
    "    if(len(notices)>0): # 해당 단어의 예상 추천 단어 추천단어로 재검색 \n",
    "        print('추천 단어로 재검색: ')\n",
    "        driver = webdriver.Chrome('C:/Users/admin/Desktop/NLP/chromedriver_v71/chromedriver', options=options)\n",
    "        driver.implicitly_wait(5)\n",
    "        url = \"https://www.google.com\"\n",
    "        driver.get(url)\n",
    "        driver.implicitly_wait(5) # 브라우저 검색 최소 지연시간   \n",
    "        elem = driver.find_element_by_xpath('//*[@id=\"tsf\"]/div[2]/div/div[1]/div/div[1]/input')\n",
    "        elem.send_keys(str(notices[0].text))\n",
    "        search_item = str(notices[0].text)\n",
    "        elem.submit()\n",
    "        driver.implicitly_wait(3) # 브라우저 검색 최소 지연시간    \n",
    "    div_group=soup.find_all(\"span\", {\"class\": \"st\"}) \n",
    "    for div in div_group: \n",
    "        try:\n",
    "            a =str(div.get_text()).replace(\"...\",\"\")\n",
    "#             temp2 = english.sub(' ',a ) #영어만 추출\n",
    "            a_temp = re.sub('[-=+,#/\\?:^$.@*\\\"※~&%ㆍ_!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》]', '', a)\n",
    "            a_temp=re.sub('\\W+','', a_temp)#두번에 걸쳐 특수문자를 제거 -> 그 후에도 남아있는 것은 영어가 아닌 단어라고 생각\n",
    "            pattern = re.compile(r'\\s+')\n",
    "            a_temp = re.sub(pattern, '', a_temp)\n",
    "            temp3 =not_english.sub('',a_temp)#영어가 아닌 단어 추출\n",
    "\n",
    "            \n",
    "            if len(temp3) > 0:#영어가 아닌 단어가 존재하면 넘어간다.\n",
    "                continue\n",
    "#             print(temp3)\n",
    "            a = re.sub('[=+#/\\?^;$.@*\\\"※~&%ㆍ_!』·“”\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》]', '', a)\n",
    "#             print(a)# 영어만 존재할 경우 append\n",
    "            print('현재 크롤링된 index:%d , 현재 단어:%s '%(idx+527,str(search_item)))\n",
    "            print('영어로된 크롤링 문장: ',a)\n",
    "            crawl_txt.append(str(a))\n",
    "                \n",
    "        except NoSuchElementException:\n",
    "                print(\"찾고자하는 검색결과가 하나도 없어서 에러남 \")\n",
    "                \n",
    "    driver.implicitly_wait(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with codecs.open(\"C:/Users/admin/Desktop/NLP/description/skipgram/train_source_target_outdata.txt\", 'w', \"utf-8\") as f:\n",
    "    for data in crawl_txt:\n",
    "        temp = str(data)+'\\n'\n",
    "        f.write(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fasttext 기존 모델을 불러와서 새로운 context학습시키기  \n",
    "cc.en.300.bin은 https://github.com/facebookresearch/fastText/blob/master/docs/crawl-vectors.md 에서 제공한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from gensim.models.fasttext import FastText \n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models.wrappers import FastText\n",
    "cap_path = datapath('C:/Users/admin/Desktop/NLP/cc.en.300.bin')\n",
    "fb_full = FastText.load_fasttext_format(cap_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000\n"
     ]
    }
   ],
   "source": [
    "print(len(fb_full.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.06489275 -0.01497137  0.08029559  0.02019559 -0.03679956]\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "print(fb_full.wv['system'][:5])\n",
    "print(len(fb_full.wv['system']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Define stabilisation stabilisation synonyms, stabilisation pronunciation, stabilisation translation, English dictionary definition of stabilisation Noun 1 stabilisation\\xa0\\n', 'the act of stabilizing something or making it more stable\\n', 'In recent years, there have been significant stabilisation interventions in places such as the Western Balkans, Haiti and Mali, among others The concept of\\xa0\\n', 'In-situ soil stabilisation is used throughout the building and civil engineering industries This is a quick and cost efficient method cheaper to modify existing soils\\xa0\\n', 'This note covers the stabilisation process, the statutory prohibitions against stabilisation and available safe harbours Free Practical Law trial To access this\\xa0\\n']\n"
     ]
    }
   ],
   "source": [
    "train_set_list=[]\n",
    "with codecs.open(\"C:/Users/admin/Desktop/NLP/description/skipgram/train_source_target_outdata.txt\", 'r', \"utf-8\") as f:\n",
    "    train_set_list= f.readlines()\n",
    "print(train_set_list[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (optional) List에 \\xa0과 \\n 제거\n",
    "##### 사실상 제거하지 않아도 무방하다. 크롤링한 txt를 그냥 사용해도 된다.\n",
    "##### 그냥 사용할 경우 새로운 word 학습 section으로 넘어가자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Define stabilisation stabilisation synonyms, stabilisation pronunciation, stabilisation translation, English dictionary definition of stabilisation Noun 1 stabilisation', 'the act of stabilizing something or making it more stable', 'In recent years, there have been significant stabilisation interventions in places such as the Western Balkans, Haiti and Mali, among others The concept of', 'In-situ soil stabilisation is used throughout the building and civil engineering industries This is a quick and cost efficient method cheaper to modify existing soils', 'This note covers the stabilisation process, the statutory prohibitions against stabilisation and available safe harbours Free Practical Law trial To access this']\n"
     ]
    }
   ],
   "source": [
    "train_set_list_=[]\n",
    "for line in train_set_list:\n",
    "    for char in ['\\xa0', '\\n']:\n",
    "        line = line.replace(char,'')\n",
    "    train_set_list_.append(line)\n",
    "print(train_set_list_[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40766\n",
      "40766\n"
     ]
    }
   ],
   "source": [
    "print(len(train_set_list_))\n",
    "print(len(train_set_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 새로운 word 학습  \n",
    "gensim에서 제공하는 fastext로는 새로운 word를 추가할 수 없다. (여러가지 에러가 발생 )  \n",
    "그래서  https://github.com/facebookresearch/fastText 를 설치하여 pretrained된 weight를 가지고 새로운 word를 추가해야한다.  \n",
    "단 facebook research에서 제공하는 code는 linux 계열 or mac환경에서만 작동된다.  \n",
    "설치 \n",
    "> 1. git clone https://github.com/facebookresearch/fastText.git\n",
    "> 2. cd fastText\n",
    "> 3. make   \n",
    "- - -\n",
    "프로그램을 설치한 후 fastText directory (fastext root) 로 들어가서 아래의 명령어를 실행한다.  \n",
    "> <pre><code>./fasttext skipgram  -input train_source_target_outdata.txt -output oov_train_model  -dim 300 -minCount 1 -verbose 2 -pretrainedVectors cc.en.300.vec</code></pre>\n",
    "- - -\n",
    "위 명령어의 의미는 https://github.com/facebookresearch/fastText.git 에서 자세한 내용은 확인할 수 있다.  \n",
    "> ###### skipgram :  \n",
    ">> cbow, skipgram, supervised가 있고 NLP 학습 모델을 무엇으로 할지 정하는 것.  \n",
    "> ###### -input : \n",
    ">> 학습시킬 data를 받는 인자로 data는 문장으로 되어있다. out of vocabrary (oov) 를 학습하기 위해  \n",
    "         크롤링을 한 data파일이 있을 것이다. 위에서 해당 문장을 train_source_target_outdata.txt 에 저장했고  \n",
    "         해당 파일을 input으로 사용하였다.  \n",
    "> ###### -output : \n",
    ">> 학습 후 나온 모델의 이름. 지정한 이름으로 두가지 파일인 .bin과 .vec 파일이 생성된다.\n",
    "> ###### -dim : \n",
    ">> word embedding을 통해 나올 단어의 차원 수. pretrained weight가 300d 였으므로 같은 차원인 300으로 맞춰준다.  \n",
    "> ###### -minCount : \n",
    ">> 최소 빈도 단어의 옵션으로 minCount보다 작은 빈도의 단어는 학습하지 않는다.  \n",
    "> ###### -pretrainedVectors : 기존의 학습된 vector로 우린 cc.en.300.vec을 사용하여 model에 넣었다. \n",
    "- - -\n",
    "해당 결과로 oov_train_model.bin과 oov_train_model.vec 파일이 생성된다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim.models.wrappers import FastText\n",
    "fb_full2 = FastText.load_fasttext_format('oov_train_model.bin') \n",
    "print(len(fb_full2.wv.vocab)) # -> 2051129 의 vocab를 갖게된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "data = np.array([1,2,3])\n",
    "for i in data:\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
